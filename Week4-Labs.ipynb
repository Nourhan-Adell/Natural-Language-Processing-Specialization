{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word Embedding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNrmiZNZwDWcT6akEZy9WRk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nourhan-Adell/Natural-language-processing/blob/main/Word_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtpAcErk_Ook",
        "outputId": "42485774-be8e-426b-96ba-0a90d70306b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[?25l\r\u001b[K     |‚ñà‚ñâ                              | 10 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñä                            | 20 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                          | 30 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                        | 40 kB 3.6 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                      | 51 kB 3.6 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                    | 61 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 71 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                 | 81 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ               | 92 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä             | 102 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå           | 112 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç         | 122 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé       | 133 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè     | 143 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 153 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 163 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 174 kB 4.1 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 175 kB 4.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=f6af60df7023116645372056ebe03c8b81da9868bbacb00e16661e81f2ef4edc\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/4e/b6/57b01db010d17ef6ea9b40300af725ef3e210cb1acfb7ac8b6\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-1.7.0\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install emoji"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize \n",
        "import emoji\n",
        "from utils2 import get_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBwv5dTw_bp4",
        "outputId": "68182c44-6cbc-4ee3-b63c-f9a351e3156c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preparation**\n",
        "**Steps**\n",
        "\n",
        "*    Clean and tokenize the corpus.\n",
        "\n",
        "*    Extract the pairs of context words and center word that will make up the training data set for the CBOW model. The context words are the features that will be fed into the model, and the center words are the target values that the model will learn to predict.\n",
        "\n",
        "*    Create simple vector representations of the context words (features) and center words (targets) that can be used by the neural network of the CBOW model.\n"
      ],
      "metadata": {
        "id": "NIVreEyZAAOC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clean and tokenization:** "
      ],
      "metadata": {
        "id": "0EK2LcS_AO4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(corpus):\n",
        "  data = re.sub(r'[,?!;-]+','.', corpus)\n",
        "  data = nltk.word_tokenize(data)\n",
        "  data = [ch.lower() for ch in data if ch.isalpha() or ch =='.' or emoji.get_emoji_regexp().search()]\n",
        "  return data"
      ],
      "metadata": {
        "id": "tJkEdoIh_6Em"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = 'I am happy because I am learning'\n",
        "print(f'Corpus:  {corpus}')\n",
        "words = tokenize(corpus)\n",
        "print('Words (tokens): ', words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xPo9OVuBMJS",
        "outputId": "b08ec982-49e7-4afc-deab-22bd2759e0c3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus:  I am happy because I am learning\n",
            "Words (tokens):  ['i', 'am', 'happy', 'because', 'i', 'am', 'learning']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sliding Window of**"
      ],
      "metadata": {
        "id": "3TSdEjRNB099"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_windows(words, c):\n",
        "  i = c\n",
        "  while i <len(words) - c:\n",
        "    center_word = words[i]\n",
        "    context_words = words[(i - c): i] + words[(i + 1): (i + c + 1)]\n",
        "    yield context_words, center_word\n",
        "    i += 1"
      ],
      "metadata": {
        "id": "DiJnYkc0BPer"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x, y in get_windows(['i', 'am', 'happy', 'because', 'i', 'am', 'learning'], 2):\n",
        "    print(f'{x}\\t{y}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9OPk3XjCWQv",
        "outputId": "32db5011-83b4-4d13-e363-63de480acb0c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'am', 'because', 'i']\thappy\n",
            "['am', 'happy', 'i', 'am']\tbecause\n",
            "['happy', 'because', 'am', 'learning']\ti\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transforming words into vectors for the training set**"
      ],
      "metadata": {
        "id": "YX4nXxlzC2Xo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transforming of the central word\n",
        "word2Ind, Ind2word = get_dict(words)\n",
        "V = len(word2Ind)\n",
        "\n",
        "def word_to_one_hot_vector(word, word2Ind, V):\n",
        "  one_hot_vector = np.zeros(V)\n",
        "  one_hot_vector[word2Ind[word]] = 1\n",
        "  return one_hot_vector"
      ],
      "metadata": {
        "id": "xA7HpOqICf-c"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word2Ind)\n",
        "print()\n",
        "word_to_one_hot_vector('happy', word2Ind, V)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lHpboy1DExG",
        "outputId": "52891cee-139a-49c0-8c37-93aefcdfb108"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'am': 0, 'because': 1, 'happy': 2, 'i': 3, 'learning': 4}\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 1., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transforming of the context words\n",
        "def context_words_to_vectors(context_words, word2Ind, V):\n",
        "  context_words_to_vectors = [word_to_one_hot_vector(w,word2Ind,V) for w in context_words]\n",
        "  context_words_to_vectors = np.mean(context_words_to_vectors, axis = 0)\n",
        "  return context_words_to_vectors"
      ],
      "metadata": {
        "id": "444GTjrBDs6R"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_words_to_vectors(['i', 'am', 'because', 'i'], word2Ind, V)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTpfDDHcE5yL",
        "outputId": "87893394-ba5c-4d61-be28-9aa3cd155f7f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.25, 0.25, 0.  , 0.5 , 0.  ])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building the training set:**"
      ],
      "metadata": {
        "id": "-t78ndJbFMc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_training_set(words, c, word2Ind, V):\n",
        "  for context_words, center_word in get_windows(words, c):\n",
        "    yield context_words_to_vectors(context_words,word2Ind, V), word_to_one_hot_vector(center_word,word2Ind, V)\n",
        "    "
      ],
      "metadata": {
        "id": "S_aGmv10E6II"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for context_words_vector, center_word_vector in get_training_set(words, 2, word2Ind, V):\n",
        "    print(f'Context words vector:  {context_words_vector}')\n",
        "    print(f'Center word vector:  {center_word_vector}')\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BdJfeEFF9lC",
        "outputId": "96bfe225-5223-4370-c50b-8329e8362b5f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context words vector:  [0.25 0.25 0.   0.5  0.  ]\n",
            "Center word vector:  [0. 0. 1. 0. 0.]\n",
            "\n",
            "Context words vector:  [0.5  0.   0.25 0.25 0.  ]\n",
            "Center word vector:  [0. 1. 0. 0. 0.]\n",
            "\n",
            "Context words vector:  [0.25 0.25 0.25 0.   0.25]\n",
            "Center word vector:  [0. 0. 0. 1. 0.]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The continous bag-of-word model:**\n",
        "**Steps:**\n",
        "\n",
        "\n",
        "*    The two activation functions used in the neural network.\n",
        "\n",
        "*    Forward propagation.\n",
        "\n",
        "*    Cross-entropy loss.\n",
        "\n",
        "*    Backpropagation.\n",
        "\n",
        "*   Gradient descent.\n",
        "\n",
        "*    Extracting the word embedding vectors from the weight matrices once the neural network has been trained.\n"
      ],
      "metadata": {
        "id": "U1QfSr-sGFI7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Activation functions:**"
      ],
      "metadata": {
        "id": "3H-qcCzIGb8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ReLU function\n",
        "def ReLU(z):\n",
        "  result = z.copy()\n",
        "  result [result < 0] = 0\n",
        "  return result"
      ],
      "metadata": {
        "id": "7PljdC3OF-Hg"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Softmax function\n",
        "def softmax(z):\n",
        "  result = np.exp(z)\n",
        "  sum_result = np.sum(result)\n",
        "  return result / sum_result"
      ],
      "metadata": {
        "id": "gey7DK24HDpC"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Forward Propagation:**"
      ],
      "metadata": {
        "id": "G4GQikYLHnCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Intializr the weights and bias matrices:\n",
        "W1 = np.array([[ 0.41687358,  0.08854191, -0.23495225,  0.28320538,  0.41800106],\n",
        "               [ 0.32735501,  0.22795148, -0.23951958,  0.4117634 , -0.23924344],\n",
        "               [ 0.26637602, -0.23846886, -0.37770863, -0.11399446,  0.34008124]])\n",
        "\n",
        "W2 = np.array([[-0.22182064, -0.43008631,  0.13310965],\n",
        "               [ 0.08476603,  0.08123194,  0.1772054 ],\n",
        "               [ 0.1871551 , -0.06107263, -0.1790735 ],\n",
        "               [ 0.07055222, -0.02015138,  0.36107434],\n",
        "               [ 0.33480474, -0.39423389, -0.43959196]])\n",
        "\n",
        "b1 = np.array([[ 0.09688219],\n",
        "               [ 0.29239497],\n",
        "               [-0.27364426]])\n",
        "\n",
        "b2 = np.array([[ 0.0352008 ],\n",
        "               [-0.36393384],\n",
        "               [-0.12775555],\n",
        "               [-0.34802326],\n",
        "               [-0.07017815]])"
      ],
      "metadata": {
        "id": "g3lyZ_9PHavY"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_set = get_training_set(words, 2, word2Ind, V)\n",
        "x_array, y_array = next(training_set)\n",
        "x = x_array.copy()\n",
        "x.shape = (V, 1)\n",
        "\n",
        "# Values of hidden layer\n",
        "z1 = np.dot(W1, x) + b1\n",
        "h =ReLU(z1)\n",
        "\n",
        "# Values of output layer\n",
        "z2 = np.dot(W2,h) + b2\n",
        "y_predict = softmax(z2)"
      ],
      "metadata": {
        "id": "2tdhVA0gIC08"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Cross-entropy loss:**"
      ],
      "metadata": {
        "id": "8S-skICiJvYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_loss(y, y_predict):\n",
        "  loss = np.sum(-np.log(y_predict) * y)\n",
        "  return loss"
      ],
      "metadata": {
        "id": "ECI3ulfLIdd7"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Back Propagation:**\n"
      ],
      "metadata": {
        "id": "NdbOrkpWKRwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = y_array.copy()\n",
        "y.shape = (V, 1)\n",
        "\n",
        "grad_b2 = y_predict - y\n",
        "\n",
        "grad_W2 = np.dot((y_predict - y), h.T)\n",
        "\n",
        "grad_b1 = ReLU(np.dot(W2.T, (y_predict - y) ))\n",
        "\n",
        "grad_W1 = np.dot(ReLU(np.dot(W2.T, (y_predict - y) )), x.T)"
      ],
      "metadata": {
        "id": "vW9Y0j99J_TL"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. Gradient Descent:**"
      ],
      "metadata": {
        "id": "jcp3ezmAMJJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpha = 0.03\n",
        "\n",
        "W1_new = W1 - (alpha * grad_W1)\n",
        "\n",
        "W2_new = W2 - (alpha * grad_W2)\n",
        "\n",
        "b1_new = b1 - (alpha * grad_b1)\n",
        "\n",
        "b2_new = b2 - (alpha * grad_b2)"
      ],
      "metadata": {
        "id": "PHYYOuQKLDp5"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. Extracting word embedding vectors:**"
      ],
      "metadata": {
        "id": "NmEXe69iMv4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Option 1: extract embedding vectors from ùêñ1\n",
        "# So the word embedding vectors corresponding to each word are\n",
        "for word in word2Ind:\n",
        "  word_embedding_vector = W1[:, word2Ind[word]]\n",
        "  print(f'{word}: {word_embedding_vector}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OY-F_j3wMfnL",
        "outputId": "fcca9828-93fc-4019-ffc9-ce84785f754d"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "am: [0.41687358 0.32735501 0.26637602]\n",
            "because: [ 0.08854191  0.22795148 -0.23846886]\n",
            "happy: [-0.23495225 -0.23951958 -0.37770863]\n",
            "i: [ 0.28320538  0.4117634  -0.11399446]\n",
            "learning: [ 0.41800106 -0.23924344  0.34008124]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Option 2: extract embedding vectors from ùêñ2\n",
        "for word in word2Ind:\n",
        "  word_embedding_vector2 = W2.T[:, word2Ind[word]]\n",
        "  print(f'{word}: {word_embedding_vector2}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLbxJLO2M5_g",
        "outputId": "4cbbe5ac-5d13-49df-8ba0-f80e20eea750"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "am: [-0.22182064 -0.43008631  0.13310965]\n",
            "because: [0.08476603 0.08123194 0.1772054 ]\n",
            "happy: [ 0.1871551  -0.06107263 -0.1790735 ]\n",
            "i: [ 0.07055222 -0.02015138  0.36107434]\n",
            "learning: [ 0.33480474 -0.39423389 -0.43959196]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Option 3: extract embedding vectors from ùêñ1 and ùêñ2\n",
        "W3 = 0.5 * (W1 + W2.T)\n",
        "\n",
        "for word in word2Ind:\n",
        "  word_embedding_vector3 = W3[:, word2Ind[word]]\n",
        "  print(f'{word}: {word_embedding_vector3}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtP-Vv7lNHph",
        "outputId": "a74ae4c3-aaf2-4461-f8aa-ab88d3429751"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "am: [ 0.09752647 -0.05136565  0.19974284]\n",
            "because: [ 0.08665397  0.15459171 -0.03063173]\n",
            "happy: [-0.02389858 -0.15029611 -0.27839106]\n",
            "i: [0.1768788  0.19580601 0.12353994]\n",
            "learning: [ 0.3764029  -0.31673866 -0.04975536]\n"
          ]
        }
      ]
    }
  ]
}
