{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week3_labs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNJEnaW9qqaLVRQkjKbmjrJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nourhan-Adell/Natural-language-processing/blob/main/Week3_labs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **N-grams Corpus preprocessing**\n",
        "Some common preprocessing steps for the language models include:\n",
        "\n",
        "*    lowercasing the text\n",
        "*    remove special characters\n",
        "*    split text to list of sentences\n",
        "*    split sentence into list words\n"
      ],
      "metadata": {
        "id": "FdYfqA3BXxt4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TiaPTt2XhVq",
        "outputId": "1d9008d6-8d3e-440e-d32c-2bd572b56425"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "import re\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Lowercasing the text:**"
      ],
      "metadata": {
        "id": "NDZD2RaiYPSw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"Learning% makes 'me' happy. I am happy be-cause I am learning! :)\"\n",
        "corpus = corpus.lower()\n",
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1q8tCMSyYHdJ",
        "outputId": "a57ebb01-8de4-43fd-a0c7-0190ae61cbaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "learning% makes 'me' happy. i am happy be-cause i am learning! :)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Remove special characters:**"
      ],
      "metadata": {
        "id": "lvSCRrVfYiyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"Learning% makes 'me' happy. I am happy be-cause I am learning! :)\"\n",
        "corpus = re.sub(r\"[^a-zA-Z0-9.?! ]+\", \"\", corpus)\n",
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Hf5SVgZYgMg",
        "outputId": "5a2100d8-e8ec-41ec-bb75-a0a479c9ce78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning makes me happy. I am happy because I am learning! \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that this process gets rid of the happy face made with punctuations :). Remember that for sentiment analysis, this emotion was very important. However, we will not consider it here.\n"
      ],
      "metadata": {
        "id": "V2pgE7R4aqte"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Text splitting:**\n"
      ],
      "metadata": {
        "id": "0CRaz36-a1zW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_date=\"Sat May  9 07:33:35 CEST 2020\"\n",
        "\n",
        "# Get the date parts in array\n",
        "date_parts = input_date.split(\" \")\n",
        "print(\"Date parts: \", date_parts)\n",
        "\n",
        "# Get the time parts in array\n",
        "time_parts = date_parts[4].split(\" \")\n",
        "print(\"Time parts: \", time_parts)\n",
        "\n",
        "print(\"Corpus split: \", corpus.split(\" \"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ig2S3-oUZBOS",
        "outputId": "d6d91346-e48a-41a0-f834-f6cbcdcd9a3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Date parts:  ['Sat', 'May', '', '9', '07:33:35', 'CEST', '2020']\n",
            "Time parts:  ['07:33:35']\n",
            "Corpus split:  ['Learning', 'makes', 'me', 'happy.', 'I', 'am', 'happy', 'because', 'I', 'am', 'learning!', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Sentence tokenizing:**\n",
        "it can be done by both split function (as done in previous cell), or by nltk library"
      ],
      "metadata": {
        "id": "ERRgbw_9cCpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'i am happy because i am learning.'\n",
        "tokenized_sentence = nltk.word_tokenize(sentence)\n",
        "print(f'{sentence} -> {tokenized_sentence}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4EkfByabWXB",
        "outputId": "23704e76-0e5b-4842-a9c0-8f36e8f0e68f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i am happy because i am learning. -> ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# find length of each word in the tokenized sentence\n",
        "word_lengths = [(word, len(word)) for word in tokenized_sentence]     #Calculate the number of characters for each word\n",
        "print(f' Lengths of the words: \\n{word_lengths}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1n4dUlrBcrHI",
        "outputId": "b575b6cd-e5a2-401b-d661-5b1acf8ad58b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Lengths of the words: \n",
            "[('i', 1), ('am', 2), ('happy', 5), ('because', 7), ('i', 1), ('am', 2), ('learning', 8), ('.', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **N-grams**\n",
        "**Sentence to n-gram.**"
      ],
      "metadata": {
        "id": "ZeQMhLoodVRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_to_trigram(tokenized_sentence):\n",
        "  for i in range(len(tokenized_sentence)-3 +1):\n",
        "    trigram = tokenized_sentence[i : i + 3]\n",
        "    print(trigram)"
      ],
      "metadata": {
        "id": "GrYxxQ7vdCaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'List all trigrams of sentence: {tokenized_sentence}\\n')\n",
        "sentence_to_trigram(tokenized_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPfSKTdKd5op",
        "outputId": "811c2af6-d2da-4764-b86f-5c64c734c5fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List all trigrams of sentence: ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n",
            "\n",
            "['i', 'am', 'happy']\n",
            "['am', 'happy', 'because']\n",
            "['happy', 'because', 'i']\n",
            "['because', 'i', 'am']\n",
            "['i', 'am', 'learning']\n",
            "['am', 'learning', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prefix of an n-gram:**\n",
        "\n",
        "\\begin{equation*}\n",
        "P(w_n|w_1^{n-1})=\\frac{C(w_1^n)}{C(w_1^{n-1})}\n",
        "\\end{equation*}\n"
      ],
      "metadata": {
        "id": "SWP3u-AneMJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# when working with trigrams, you need to prepend 2 <s> and append one </s>\n",
        "n = 3\n",
        "tokenized_sentence = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n",
        "tokenized_sentence = [\"<s>\"] * (n - 1) + tokenized_sentence + [\"<e>\"]\n",
        "print(tokenized_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8T2GC-_eEzi",
        "outputId": "c951f303-903b-4887-ef49-44afd39f3190"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s>', '<s>', 'i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.', '<e>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Thus, to allow the equation to work properly**"
      ],
      "metadata": {
        "id": "LIJYhAWteqLF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lab2-Building the language model**"
      ],
      "metadata": {
        "id": "7abPb6DjfBMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# manipulate n_gram count dictionary\n",
        "\n",
        "n_gram_counts = {\n",
        "    ('i', 'am', 'happy'): 2,\n",
        "    ('am', 'happy', 'because'): 1}\n",
        "\n",
        "# get count for an n-gram tuple\n",
        "print(f\"count of n-gram {('i', 'am', 'happy')}: {n_gram_counts[('i', 'am', 'happy')]}\")\n",
        "\n",
        "# check if n-gram is present in the dictionary\n",
        "if ('i', 'am', 'learning') in n_gram_counts:\n",
        "    print(f\"n-gram {('i', 'am', 'learning')} found\")\n",
        "else:\n",
        "    print(f\"n-gram {('i', 'am', 'learning')} missing\")\n",
        "\n",
        "# update the count in the word count dictionary\n",
        "n_gram_counts[('i', 'am', 'learning')] = 1\n",
        "if ('i', 'am', 'learning') in n_gram_counts:\n",
        "    print(f\"n-gram {('i', 'am', 'learning')} found\")\n",
        "else:\n",
        "    print(f\"n-gram {('i', 'am', 'learning')} missing\")\n",
        "\n",
        "print(n_gram_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jYHFf2pe_-j",
        "outputId": "06c572df-87c4-4bd1-ce10-7672a278397f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count of n-gram ('i', 'am', 'happy'): 2\n",
            "n-gram ('i', 'am', 'learning') missing\n",
            "n-gram ('i', 'am', 'learning') found\n",
            "{('i', 'am', 'happy'): 2, ('am', 'happy', 'because'): 1, ('i', 'am', 'learning'): 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **the count matrix could be made in a single pass through the corpus**"
      ],
      "metadata": {
        "id": "E3EgsjGvf24z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "def single_pass_trigram_count_matrix(corpus):\n",
        "    \"\"\"\n",
        "    Creates the trigram count matrix from the input corpus in a single pass through the corpus.\n",
        "    \n",
        "    Args:\n",
        "        corpus: Pre-processed and tokenized corpus. \n",
        "    \n",
        "    Returns:\n",
        "        bigrams: list of all bigram prefixes, row index\n",
        "        vocabulary: list of all found words, the column index\n",
        "        count_matrix: pandas dataframe with bigram prefixes as rows, \n",
        "                      vocabulary words as columns \n",
        "                      and the counts of the bigram/word combinations (i.e. trigrams) as values\n",
        "    \"\"\"\n",
        "    bigrams = []\n",
        "    vocabulary = []\n",
        "    count_matrix_dict = defaultdict(dict)\n",
        "    \n",
        "    # go through the corpus once with a sliding window\n",
        "    for i in range(len(corpus) - 3 + 1):\n",
        "        # the sliding window starts at position i and contains 3 words\n",
        "        trigram = tuple(corpus[i : i + 3])\n",
        "        \n",
        "        bigram = trigram[0 : -1]\n",
        "        if not bigram in bigrams:\n",
        "            bigrams.append(bigram)        \n",
        "        \n",
        "        last_word = trigram[-1]\n",
        "        if not last_word in vocabulary:\n",
        "            vocabulary.append(last_word)\n",
        "        \n",
        "        if (bigram,last_word) not in count_matrix_dict:\n",
        "            count_matrix_dict[bigram,last_word] = 0\n",
        "            \n",
        "        count_matrix_dict[bigram,last_word] += 1\n",
        "    \n",
        "    # convert the count_matrix to np.array to fill in the blanks\n",
        "    count_matrix = np.zeros((len(bigrams), len(vocabulary)))\n",
        "    for trigram_key, trigam_count in count_matrix_dict.items():\n",
        "        count_matrix[bigrams.index(trigram_key[0]), \\\n",
        "                     vocabulary.index(trigram_key[1])]\\\n",
        "        = trigam_count\n",
        "    \n",
        "    # np.array to pandas dataframe conversion\n",
        "    count_matrix = pd.DataFrame(count_matrix, index=bigrams, columns=vocabulary)\n",
        "    return bigrams, vocabulary, count_matrix"
      ],
      "metadata": {
        "id": "eabDr5-nfVEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n",
        "\n",
        "bigrams, vocabulary, count_matrix = single_pass_trigram_count_matrix(corpus)\n",
        "\n",
        "print(count_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2B3JMNLOgEE1",
        "outputId": "a3c41516-3ffb-4211-f077-ea911e62d225"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  happy  because    i   am  learning    .\n",
            "(i, am)             1.0      0.0  0.0  0.0       1.0  0.0\n",
            "(am, happy)         0.0      1.0  0.0  0.0       0.0  0.0\n",
            "(happy, because)    0.0      0.0  1.0  0.0       0.0  0.0\n",
            "(because, i)        0.0      0.0  0.0  1.0       0.0  0.0\n",
            "(am, learning)      0.0      0.0  0.0  0.0       0.0  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Probability matrix:**"
      ],
      "metadata": {
        "id": "6XcF_fVAj6nP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create the probability matrix from the count matrix\n",
        "row_sums = count_matrix.sum(axis=1)\n",
        "# divide each row by its sum\n",
        "prob_matrix = count_matrix.div(row_sums, axis=0)\n",
        "\n",
        "print(prob_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUXuXQM-in3q",
        "outputId": "c99560db-ca01-4a70-a588-e02553126d91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  happy  because    i   am  learning    .\n",
            "(i, am)             0.5      0.0  0.0  0.0       0.5  0.0\n",
            "(am, happy)         0.0      1.0  0.0  0.0       0.0  0.0\n",
            "(happy, because)    0.0      0.0  1.0  0.0       0.0  0.0\n",
            "(because, i)        0.0      0.0  0.0  1.0       0.0  0.0\n",
            "(am, learning)      0.0      0.0  0.0  0.0       0.0  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# find the probability of a trigram in the probability matrix\n",
        "trigram = ('i', 'am', 'happy')\n",
        "\n",
        "# find the prefix bigram \n",
        "bigram = trigram[:-1]\n",
        "print(f'bigram: {bigram}')\n",
        "\n",
        "# find the last word of the trigram\n",
        "word = trigram[-1]\n",
        "print(f'word: {word}')\n",
        "\n",
        "trigram_probability = prob_matrix[word][bigram]\n",
        "print(f'trigram_probability: {trigram_probability}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vag7NN7BkDg1",
        "outputId": "d8604d47-1d63-4cf9-daae-e5622264a12d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bigram: ('i', 'am')\n",
            "word: happy\n",
            "trigram_probability: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Language model evaluation:**"
      ],
      "metadata": {
        "id": "NgwVmkaHkdqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "def train_validation_test_split(data, train_precent, validation_precent):\n",
        "  # fixed seed here for reproducibility\n",
        "  random.seed(87)\n",
        "  # reshuffle all input sentences\n",
        "  random.shuffle(data)\n",
        "  train_size = int(len(data) * train_precent / 100)\n",
        "  train_data = data[0:train_size]\n",
        "\n",
        "  validation_size = int(len(data) * validation_precent / 100)\n",
        "  validation_data = data[train_size: train_size + validation_size]\n",
        "\n",
        "  test_data = data[train_size + validation_size : ]\n",
        "  return train_data, validation_data, test_data"
      ],
      "metadata": {
        "id": "6gzOkc0skMqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [x for x in range (0, 100)]\n",
        "\n",
        "train_data, validation_data, test_data = train_validation_test_split(data, 80, 10)\n",
        "print(\"split 80/10/10:\\n\",f\"train data:{train_data}\\n\", f\"validation data:{validation_data}\\n\", \n",
        "      f\"test data:{test_data}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44yep1I8l-qg",
        "outputId": "cf8c1305-9e04-49c5-d87e-ebea26f62d44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "split 80/10/10:\n",
            " train data:[28, 76, 5, 0, 62, 29, 54, 95, 88, 58, 4, 22, 92, 14, 50, 77, 47, 33, 75, 68, 56, 74, 43, 80, 83, 84, 73, 93, 66, 87, 9, 91, 64, 79, 20, 51, 17, 27, 12, 31, 67, 81, 7, 34, 45, 72, 38, 30, 16, 60, 40, 86, 48, 21, 70, 59, 6, 19, 2, 99, 37, 36, 52, 61, 97, 44, 26, 57, 89, 55, 53, 85, 3, 39, 10, 71, 23, 32, 25, 8]\n",
            " validation data:[78, 65, 63, 11, 49, 98, 1, 46, 15, 41]\n",
            " test data:[90, 96, 82, 42, 35, 13, 69, 24, 94, 18]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Perplexity:**\n",
        "\n",
        "\\begin{equation*}\n",
        "PP(W)=\\sqrt[M]{\\prod_{i=1}^{m}{\\frac{1}{P(w_i|w_{i-1})}}}\n",
        "\\end{equation*}\n",
        "\n",
        "Remember from calculus:\n",
        "\n",
        "\\begin{equation*}\n",
        "\\sqrt[M]{\\frac{1}{x}} = x^{-\\frac{1}{M}}\n",
        "\\end{equation*}\n"
      ],
      "metadata": {
        "id": "jZDKGeUjmOuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to calculate the exponent, use the following syntax\n",
        "p = 10 ** (-250)\n",
        "M = 100\n",
        "perplexity = p ** (-1 / M)\n",
        "print(perplexity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfPWe9h8mDMZ",
        "outputId": "51668cf1-c6a7-4a71-c997-a567cf8d4fe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "316.22776601683796\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lab3-Language model generalization:**\n",
        "Out of vocabulary words (OOV)"
      ],
      "metadata": {
        "id": "uvcwelEO0enY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the vocabulary from M most frequent words\n",
        "from collections import Counter\n",
        "\n",
        "M = 3\n",
        "word_counts = {'happy':5, \"because\":2, 'i':2, 'am':2, 'learning':3, '.':1}\n",
        "vocabulary = Counter(word_counts).most_common(M)\n",
        "# remove the frequencies and leave just the words\n",
        "vocabulary = [w[0] for w in vocabulary]\n",
        "print(f\"the new vocabulary containing {M} most frequent words: {vocabulary}\\n\") \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3MpM2921sva",
        "outputId": "8aa3b18b-8bc3-4e99-a8a6-e6c066356e0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the new vocabulary containing 3 most frequent words: ['happy', 'learning', 'because']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = ['am','i','learning']\n",
        "output_sentence = []\n",
        "for word in sentence:\n",
        "  if word in vocabulary:\n",
        "    output_sentence.append(word)\n",
        "  else:\n",
        "    output_sentence.append('<UNK>')\n",
        "\n",
        "print(f\"input sentence: {sentence}\")\n",
        "print('output sentence: ', output_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xz2YY_y82VRg",
        "outputId": "964d9745-6d3f-4f1a-e91d-9a66045fe9f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input sentence: ['am', 'i', 'learning']\n",
            "output sentence:  ['<UNK>', '<UNK>', 'learning']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iterate through all word counts and print words with given frequency f\n",
        "\n",
        "f = 3\n",
        "word_counts = {'happy': 5, 'because': 3, 'i': 2, 'am': 2, 'learning':3, '.': 1}\n",
        "for word,freq in word_counts.items():\n",
        "  if freq == f:\n",
        "    print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtB47vi23Aag",
        "outputId": "7d208cf8-2b40-4537-ed35-acebab972ac3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "because\n",
            "learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Smoothing:"
      ],
      "metadata": {
        "id": "Z-RsDE_a4CSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_k_smooting_probability(k, vocabulary_size, n_gram_count, n_gram_prefix_count):\n",
        "    numerator = n_gram_count + k\n",
        "    denominator = n_gram_prefix_count + k * vocabulary_size\n",
        "    return numerator / denominator"
      ],
      "metadata": {
        "id": "7RIcEb2N3U_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "48-Ov4bx4BV3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
