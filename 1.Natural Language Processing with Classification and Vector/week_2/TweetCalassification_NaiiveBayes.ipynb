{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOW9Skiuap7cFru7ZuoraWj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nourhan-Adell/Natural-Language-Processing-Specialization/blob/main/1.Natural%20Language%20Processing%20with%20Classification%20and%20Vector/week_2/TweetCalassification_NaiiveBayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Classify tweets by Naive Bayes algorithm**"
      ],
      "metadata": {
        "id": "VpJktJsuNi1a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5G8iFYINW25",
        "outputId": "896a674f-b679-4e66-f383-98bbe6cbf0ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import pdb\n",
        "from nltk.corpus import stopwords, twitter_samples\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import string\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from os import getcwd\n",
        "\n",
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Get and inspect the data**"
      ],
      "metadata": {
        "id": "ZautlRU7bkFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filePath = f\"{getcwd()}/week2/\"\n",
        "nltk.data.path.append(filePath)"
      ],
      "metadata": {
        "id": "Avs8qhKfN27D"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the sets of positive and negative tweets\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "\n",
        "# split the data into two pieces, one for training and one for testing (validation set)\n",
        "train_pos = all_positive_tweets[:4000]\n",
        "train_neg = all_negative_tweets[:4000]\n",
        "\n",
        "test_pos = all_positive_tweets[4000:]\n",
        "test_neg = all_negative_tweets[4000:]\n",
        "\n",
        "train_x = train_pos + train_neg\n",
        "test_x = test_pos + test_neg\n",
        "\n",
        "# avoid assumptions about the length of all_positive_tweets\n",
        "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
        "test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))"
      ],
      "metadata": {
        "id": "9O-WyP5bbEho"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Preprocessing the data**"
      ],
      "metadata": {
        "id": "boXyDG9sbphF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import re\n",
        "import string\n",
        "\n",
        "def preprocess_tweets(tweet):\n",
        "  # stem the tweet(r3eturn it to its origin)\n",
        "  stemmer = PorterStemmer()\n",
        "\n",
        "  # Remove the stope words and signs\n",
        "  stopwords_english = stopwords.words('english')\n",
        "  # remove stock market tickers like $GE\n",
        "  tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "  # remove old style retweet text \"RT\"\n",
        "  tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "  # remove hyperlinks\n",
        "  #tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "  tweet = re.sub(r'https?://[^\\s\\n\\r]+', '', tweet)\n",
        "  # remove hashtags\n",
        "  tweet = re.sub(r'#', '', tweet)\n",
        "\n",
        "  # tokenize tweets\n",
        "  tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
        "                               reduce_len=True)\n",
        "  tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "  tweets_clean = []\n",
        "  for word in tweet_tokens:\n",
        "      if (word not in stopwords_english and  # remove stopwords\n",
        "          word not in string.punctuation):  # remove punctuation\n",
        "          # tweets_clean.append(word)\n",
        "          stem_word = stemmer.stem(word)  # stemming word\n",
        "          tweets_clean.append(stem_word)\n",
        "  return tweets_clean"
      ],
      "metadata": {
        "id": "k-WxlifrbINh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count tweets function to calculae the frequencies\n",
        "\n",
        "def count_tweets(result, tweets, ys):\n",
        "  '''\n",
        "    Input:\n",
        "        result: a dictionary that will be used to map each pair to its frequency\n",
        "        tweets: a list of tweets\n",
        "        ys: a list corresponding to the sentiment of each tweet (either 0 or 1)\n",
        "    Output:\n",
        "        result: a dictionary mapping each pair to its frequency\n",
        "    '''\n",
        "  for y, tweet in zip(ys, tweets):\n",
        "    for word in preprocess_tweets(tweet):\n",
        "      pair = (word, y)\n",
        "      \n",
        "      if pair in result:\n",
        "        result[pair] += 1\n",
        "      else:\n",
        "        result[pair] = 1\n",
        "  \n",
        "  return result"
      ],
      "metadata": {
        "id": "BrBl0pcZcydz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lookup(freqs, word, label):\n",
        "  n = 0\n",
        "  pair = (word, label)\n",
        "  if pair in freqs:\n",
        "    n = freqs[pair]\n",
        "  return n"
      ],
      "metadata": {
        "id": "PbLMkPWGfxYx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Train the Naive Bayes model**"
      ],
      "metadata": {
        "id": "g4SYvZXPd4ot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results ={}\n",
        "freqs = count_tweets(results, train_x, train_y)"
      ],
      "metadata": {
        "id": "NHzRO-tvdwqu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_NaiveBayes_model(freqs, train_x, train_y):\n",
        "  logprior = 0\n",
        "  loglikelihood = {}\n",
        "\n",
        "  vocab = set([pair[0] for pair in freqs.keys()])\n",
        "  V = len(vocab)\n",
        "\n",
        "  # calculate N_pos, N_neg, V_pos, V_neg\n",
        "  N_pos = N_neg = 0\n",
        "  for pair in freqs.keys():\n",
        "    if pair[1]> 0:\n",
        "      N_pos += freqs[pair]\n",
        "    else:\n",
        "      N_neg += freqs[pair]\n",
        "\n",
        "  # Calculate D, the number of documents\n",
        "  D = len(train_y)\n",
        "  D_pos = len(list(filter(lambda x: x > 0, train_y)))\n",
        "  D_neg = len(list(filter(lambda x: x <= 0, train_y)))\n",
        "\n",
        "  # Calculate logprior\n",
        "  logprior = np.log(D_pos) - np.log(D_neg)\n",
        "\n",
        "  for word in vocab:\n",
        "    freq_pos = lookup(freqs, word, 1)\n",
        "    freq_neg = lookup(freqs, word, 0)\n",
        "\n",
        "    # calculate the probability that each word is positive, and negative with Laplacian smoothing\n",
        "    prob_word_pos = (freq_pos + 1) / (N_pos + V)\n",
        "    prob_word_neg = (freq_neg + 1) / (N_neg + V)\n",
        "    \n",
        "    # calculate the log likelihood of the word\n",
        "    loglikelihood[word] = np.log (prob_word_pos / prob_word_neg)\n",
        "  \n",
        "  return logprior, loglikelihood"
      ],
      "metadata": {
        "id": "tABAPVhfeTTD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logprior, loglikelihood = train_NaiveBayes_model(freqs, train_x, train_y)\n",
        "print(logprior)\n",
        "print(len(loglikelihood))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ls3e74-dg6L0",
        "outputId": "74ef1479-9312-4383-f0a7-a37bd71f05e1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n",
            "9162\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Test the Naive Bayes model**"
      ],
      "metadata": {
        "id": "Gh6bf5gyhITJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def NaiveBayes_predict(tweet, logprior, loglikelihood):\n",
        "  word_list = preprocess_tweets(tweet)\n",
        "  prob = 0 \n",
        "  prob += logprior\n",
        "\n",
        "  for word in word_list:\n",
        "    if word in loglikelihood:\n",
        "      prob += loglikelihood[word]\n",
        "  \n",
        "  return prob"
      ],
      "metadata": {
        "id": "TOoLwlLghDLO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the result of the prob > 1, so it's a positive sentiment and vice versa"
      ],
      "metadata": {
        "id": "I1n58GzjiSlF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_NaiveBayes_model(test_x, test_y, logprior, loglikelihood, NaiveBayes_predict = NaiveBayes_predict):\n",
        "  accuracy = 0\n",
        "  y_predict = []\n",
        "\n",
        "  for tweet in test_x:\n",
        "    if NaiveBayes_predict(tweet, logprior, loglikelihood) > 0:\n",
        "      y_predict_result = 1\n",
        "    else:\n",
        "      y_predict_result = 0\n",
        "    y_predict.append(y_predict_result)\n",
        "\n",
        "  # Calculate the mean absolute error\n",
        "  error = np.mean(np.abs(y_predict - test_y))\n",
        "  accuracy = 1 - error\n",
        "\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "U1URpXd1iGn1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Naive Bayes accuracy = %0.4f\" %(test_NaiveBayes_model(test_x, test_y, logprior, loglikelihood)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXBBNRDijWhA",
        "outputId": "cb7e9dc2-a588-4098-a0be-d786ec485767"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes accuracy = 0.9955\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# So, the Naive Bayes Model works with tweets and gives accuracy = 99.5%"
      ],
      "metadata": {
        "id": "CxVUSlZaj10R"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iX-CBJcJj-I_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}